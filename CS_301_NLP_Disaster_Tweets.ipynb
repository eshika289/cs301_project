{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS 301 - NLP - Disaster Tweets",
      "provenance": [],
      "collapsed_sections": [
        "3MZjlumnpEKm",
        "5OEH3tR8n8XD",
        "1HvJN22spr7-",
        "Ws-LiX-KoDGj",
        "E6iYA5pgwEd5",
        "Lf8CA4ZozCcF",
        "-rArfTFo2I7Y",
        "vgNrFYDM1vVr",
        "-na0Idv44SQO",
        "-sWStJ_P9_lf"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN5P7HPb+gTEGoMq9w9e6Uu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eshika289/cs301_project/blob/main/CS_301_NLP_Disaster_Tweets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Statements"
      ],
      "metadata": {
        "id": "3MZjlumnpEKm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.numpy as jnp\n",
        "from jax import grad, jit, vmap\n",
        "from jax import random\n",
        "import jax"
      ],
      "metadata": {
        "id": "APL0P06erOf-"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://jax.readthedocs.io/en/latest/notebooks/quickstart.html"
      ],
      "metadata": {
        "id": "aHrLY07ZwuRq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, shutil\n",
        "import csv\n",
        "import re\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "XeGKCQKjxAD9"
      },
      "execution_count": 256,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Union\n",
        "from typing import Any\n",
        "from typing import Dict\n",
        "from typing import TypeVar\n",
        "from typing import NamedTuple, Optional"
      ],
      "metadata": {
        "id": "U4NCEisb0m2T"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet as wn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42_ZfksrqZ9F",
        "outputId": "cb16e5fa-93e8-4b63-8e97-f4bf1cbd2208"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get Data From Kaggle"
      ],
      "metadata": {
        "id": "5OEH3tR8n8XD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get kaggle.json file from kaggle and upload it to the content folder"
      ],
      "metadata": {
        "id": "ptMx-wYPrgqr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd # /content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ak2cGh9prm06",
        "outputId": "d9c62fd4-9ecb-494a-dd97-5492275f1ff7"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /root/.kaggle\n",
        "!mkdir kaggle"
      ],
      "metadata": {
        "id": "03d8Hklgrpxi"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv ./kaggle.json /root/.kaggle/kaggle.json\n",
        "!cat /root/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "z51VyhCursMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install kaggle\n",
        "!kaggle -v"
      ],
      "metadata": {
        "id": "y1g6tHWXslah",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1603780e-8e0f-44a4-cd5f-5202f1421079"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "Kaggle API 1.5.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod 600 /root/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "KuQSB3IGsr7p"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions files -c nlp-getting-started"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCqsnXy_tE3M",
        "outputId": "e402792d-4b39-49c9-ad61-3b58a109241f"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name                    size  creationDate         \n",
            "---------------------  -----  -------------------  \n",
            "train.csv              965KB  2019-12-16 20:36:20  \n",
            "test.csv               411KB  2019-12-16 20:36:20  \n",
            "sample_submission.csv   22KB  2019-12-16 20:36:20  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions download -c nlp-getting-started -p /content/kaggle/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f43IO7CTsxjK",
        "outputId": "b993e697-0a82-4859-d59e-23d0a6c6a1ab"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading nlp-getting-started.zip to /content/kaggle\n",
            "\r  0% 0.00/593k [00:00<?, ?B/s]\n",
            "\r100% 593k/593k [00:00<00:00, 104MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q /content/kaggle/nlp-getting-started.zip -d /content/kaggle/data"
      ],
      "metadata": {
        "id": "V3ET8MSetuk2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd1d6fe3-048a-4e67-f1a7-5f34fbb16e72"
      },
      "execution_count": 360,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "replace /content/kaggle/data/sample_submission.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: no\n",
            "replace /content/kaggle/data/test.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: no\n",
            "replace /content/kaggle/data/train.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: no\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = \"kaggle/data/train.csv\"\n",
        "data_file = open(data_path)\n",
        "read_file = csv.reader(data_file)\n",
        "headers = []\n",
        "headers = next(read_file)\n",
        "data = []\n",
        "for row in read_file:\n",
        "  data.append(row)"
      ],
      "metadata": {
        "id": "3FBaTgM2mrRK"
      },
      "execution_count": 394,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(headers) # ['id', 'keyword', 'location', 'text', 'target']\n",
        "print(data[0])\n",
        "print(len(data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3IEM4-mmxxMX",
        "outputId": "a9208fbb-e1fd-4b65-c749-c17c6f199231"
      },
      "execution_count": 395,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['id', 'keyword', 'location', 'text', 'target']\n",
            "['1', '', '', 'Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all', '1']\n",
            "7613\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Features"
      ],
      "metadata": {
        "id": "1HvJN22spr7-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Most Common Words as Features"
      ],
      "metadata": {
        "id": "Ws-LiX-KoDGj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def most_common_words(subset):\n",
        "  all_text = \"\"\n",
        "  for i in range(len(subset)):\n",
        "    all_text+=subset[i][3]+\" \"\n",
        "\n",
        "  stop_words = stopwords.words('english')\n",
        "  all_text.lower()\n",
        "  words_list = all_text.split()\n",
        "  word_list = []\n",
        "  part_of_speech = {}\n",
        "\n",
        "  for word in words_list:\n",
        "    if word not in stop_words and re.search('[a-z]', word) is not None and word!=\"&amp;\" and word[:4]!=\"http\":\n",
        "      #tmp = wn.synsets(word)[0].pos()\n",
        "      word_list.append(word.lower())\n",
        "      #part_of_speech[word.lower]= tmp\n",
        "\n",
        "  #words_dict = dict(sorted(words_dict.items(), key=lambda item: -item[1]))\n",
        "  words_dict = Counter(word_list)\n",
        "  for item in words_dict.most_common(50):\n",
        "    print(item)\n",
        "    #print(part_of_speech[item])\n",
        "\n",
        "most_common_words(data)"
      ],
      "metadata": {
        "id": "6YVXP_VfrV2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bigrams / Trigrams As Features"
      ],
      "metadata": {
        "id": "E6iYA5pgwEd5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "O87hFa0FwDXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grammars (nouns,verbs) As Features"
      ],
      "metadata": {
        "id": "BcEqjwo9xt4p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "GHEcwKh6yAes"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Tree"
      ],
      "metadata": {
        "id": "smAoYiMPyB-N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entropy"
      ],
      "metadata": {
        "id": "Lf8CA4ZozCcF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def entropy(class_probabilities: List[float]) -> float:\n",
        "  '''Given a list of class probabilities, compute the entropy'''\n",
        "  return jnp.sum(jnp.asarray([-p*jnp.log2(p) for p in class_probabilities if p > 0]))\n",
        "\n",
        "def class_probabilities(labels: List[Any]) -> float:\n",
        "  total_count = jnp.size(labels)\n",
        "  return [count/total_count for count in Counter(labels).values()]\n",
        "\n",
        "def data_entropy(labels: List[Any]) -> float:\n",
        "  return entropy(class_probabilities(labels))\n",
        "\n",
        "def partition_entropy(subsets: List[List[Any]]) -> float:\n",
        "  '''Returns the entropy from this partition of data into subsets'''\n",
        "  total_count = jnp.sum(jnp.asarray([jnp.size(subset) for subset in subsets]))\n",
        "  return jnp.sum(jnp.asarray([data_entropy(subset)*jnp.size(subset)/total_count for subset in subsets]))"
      ],
      "metadata": {
        "id": "g3jcrUV4yxrr"
      },
      "execution_count": 343,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parition"
      ],
      "metadata": {
        "id": "-rArfTFo2I7Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "T = TypeVar('T')\n",
        "\n",
        "def partition_by(inputs: List[T], attribute:str) -> Dict[Any, List[T]]:\n",
        "  '''Partitions the inputs into lists based on the specified attribute'''\n",
        "  partitions: Dict[Any,List[T]] = defaultdict(list)\n",
        "  for input in inputs:\n",
        "    key = getattr(input, attribute)\n",
        "    partitions[key].append(input)\n",
        "  return partitions\n",
        "\n",
        "def partition_entropy_by(inputs: List[Any], attribute:str,label_attribute:str) ->float:\n",
        "  '''Computes the entropy corresponding to the given partition'''\n",
        "  partitions = partition_by(inputs,attribute)\n",
        "  labels = [[getattr(input, label_attribute) for input in partition] for partition in partitions.values()]\n",
        "  return partition_entropy(labels)"
      ],
      "metadata": {
        "id": "TUFMpSL72HAV"
      },
      "execution_count": 304,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Entropy and Partition"
      ],
      "metadata": {
        "id": "vgNrFYDM1vVr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(headers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9q262QITwIi",
        "outputId": "a1fe5b31-efed-4ef9-be70-402067b3de82"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['id', 'keyword', 'location', 'text', 'target']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweets = []\n",
        "\n",
        "class Tweet(NamedTuple):\n",
        "  keyword: str\n",
        "  location: str\n",
        "  target: int\n",
        "\n",
        "for tweet in train_data:\n",
        "  tweets.append(Tweet(tweet[1],tweet[2],tweet[4]))"
      ],
      "metadata": {
        "id": "zM_g0CyWTqQO"
      },
      "execution_count": 389,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(tweets))\n",
        "print(len(train_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DxHy7u2mVyXA",
        "outputId": "9450fdd6-e59d-409f-f80c-49b896c8337e"
      },
      "execution_count": 390,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5000\n",
            "5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for key in ['keyword','location']:\n",
        "  print(key,partition_entropy_by(tweets,key,'target'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxuyEKSQWSYr",
        "outputId": "307eadad-0cd5-4eb2-e8b9-03ef4bf33eeb"
      },
      "execution_count": 375,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "keyword 0.7342361\n",
            "location 0.5156009\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build Tree"
      ],
      "metadata": {
        "id": "-na0Idv44SQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Leaf(NamedTuple):\n",
        "  value: Any\n",
        "\n",
        "class Split(NamedTuple):\n",
        "  attribute: str\n",
        "  subtrees: dict\n",
        "  default_value: Any = None\n",
        "\n",
        "DecisionTree = Union[Leaf,Split]"
      ],
      "metadata": {
        "id": "454J3DuR28o8"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Tweet(NamedTuple):\n",
        "  keyword: str\n",
        "  location: str\n",
        "  target: int"
      ],
      "metadata": {
        "id": "uhZgO9ux9Svn"
      },
      "execution_count": 391,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classify(tree:DecisionTree,input:Any) -> Any:\n",
        "  '''Classify the input using the given decision tree'''\n",
        "  \n",
        "  # If this is a leaf node, return its value\n",
        "  if isinstance(tree,Leaf):\n",
        "    return tree.value\n",
        "\n",
        "  subtree_key = getattr(input, tree.attribute)\n",
        "\n",
        "  if subtree_key not in tree.subtrees:\n",
        "    return tree.default_value\n",
        "  \n",
        "  subtree = tree.subtrees[subtree_key]\n",
        "  return classify(subtree,input)"
      ],
      "metadata": {
        "id": "nDiHJANM4BrT"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_tree_id3(inputs: List[Any],split_attributes:List[str],target_attribute:str)->DecisionTree:\n",
        "  # Count target labels\n",
        "  label_counts = Counter(getattr(input,target_attribute) for input in inputs)\n",
        "  most_common_label = label_counts.most_common(1)[0][0]\n",
        "\n",
        "  # If there's a unique label, predict it\n",
        "  if len(label_counts) == 1:\n",
        "    return Leaf(most_common_label)\n",
        "  \n",
        "  # If no split attributes left, return the majority label\n",
        "  if not split_attributes:\n",
        "    return Leaf(most_common_label)\n",
        "\n",
        "  # Otherwise split by the best attribute\n",
        "  def split_entropy(attribute:str) -> float:\n",
        "    return partition_entropy_by(inputs,attribute,target_attribute)\n",
        "  \n",
        "  best_attribute = min(split_attributes, key=split_entropy)\n",
        "\n",
        "  partitions = partition_by(inputs, best_attribute)\n",
        "  new_attributes = [a for a in split_attributes if a !=best_attribute]\n",
        "\n",
        "  # Recursively build subtrees\n",
        "  subtrees = {attribute_value: build_tree_id3(subset,new_attributes, target_attribute) for attribute_value,subset in partitions.items()}\n",
        "\n",
        "  return Split(best_attribute, subtrees, default_value=most_common_label)"
      ],
      "metadata": {
        "id": "QcWju7rK5sEG"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Build Tree"
      ],
      "metadata": {
        "id": "-sWStJ_P9_lf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decision_tree = build_tree_id3(tweets,['keyword','location'],'target')\n",
        "#print(decision_tree)"
      ],
      "metadata": {
        "id": "zFJILcu2WlTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Accuracy of Tree"
      ],
      "metadata": {
        "id": "KYOXMOUHkgQe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(decision_tree,tweets):\n",
        "  correct = 0\n",
        "  incorrect = 0\n",
        "  for tweet in tweets:\n",
        "    predictedOutput(decision_tree,tweet)\n",
        "    if value== tweet.target:\n",
        "      correct+=1\n",
        "    else:\n",
        "      incorrect+=1\n",
        "  #print(\"Correct: \", correct)\n",
        "  #print(\"Incorrect: \", incorrect)\n",
        "\n",
        "  accuracy = correct/(correct+incorrect)\n",
        "  #print(\"Accuracy: \", accuracy)\n",
        "  return accuracy\n",
        "\n",
        "\n",
        "def predictedOutput(decision_tree,tweet):\n",
        "  ''' will use the decision_tree to predict if tweet is valid or not\n",
        "      changes the global variable value to 1 for valid and 0 for invalid\n",
        "  '''\n",
        "  global value\n",
        "  tree= decision_tree\n",
        "  attribute = tree[0]\n",
        "  tweet_val = None\n",
        "  if attribute == 'location':\n",
        "    tweet_val = tweet.location\n",
        "  if attribute == 'keyword':\n",
        "    tweet_val = tweet.keyword\n",
        "  try:\n",
        "    tree = tree[1][tweet_val]\n",
        "    if isinstance(tree,Leaf):\n",
        "        value = tree.value\n",
        "    else:\n",
        "      predictedOutput(tree,tweet)\n",
        "  except:\n",
        "    value = tree[2]"
      ],
      "metadata": {
        "id": "8cmwiNChd09V"
      },
      "execution_count": 482,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy(decision_tree,tweets)\n",
        "#predictedOutput(decision_tree,tweets[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQ81UTqSpb_I",
        "outputId": "d58393e4-79f6-4536-938f-8099d79bee2f"
      },
      "execution_count": 483,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9218"
            ]
          },
          "metadata": {},
          "execution_count": 483
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Dataset Accuracy"
      ],
      "metadata": {
        "id": "wKVM3KTDkpyT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_tweets = []\n",
        "for tweet in test_data:\n",
        "  test_tweets.append(Tweet(tweet[1],tweet[2],tweet[4]))\n",
        "\n",
        "print(len(test_data))\n",
        "print(len(test_tweets))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vD4_exIWm7dp",
        "outputId": "ddd8c7f6-7477-4f40-f222-f7dd66b764fa"
      },
      "execution_count": 463,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2613\n",
            "2613\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy(decision_tree,test_tweets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-E2kh6tlm1gi",
        "outputId": "68d66a29-4e21-4816-8037-a5a01c0c2399"
      },
      "execution_count": 484,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8143895905089935"
            ]
          },
          "metadata": {},
          "execution_count": 484
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions to Use to Build Trees "
      ],
      "metadata": {
        "id": "kVVNPZ-sD7n3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   tree(train_data,test_data) -> returns a decision tree\n",
        "*   predictedOutput(tree,tweet) -> changes the global variable value to 1 if valid and 0 if not valid\n",
        "*   decision_tree_accuracies(tree,train_tweets,test_tweets) -> returns the train and test accuracies as a tuple (train_accuracy,test_accuracy)\n"
      ],
      "metadata": {
        "id": "pvPt757vFRyk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tree(train_data,test_data):\n",
        "  global train_tweets\n",
        "  global test_tweets\n",
        "  train_tweets = []\n",
        "  test_tweets = []\n",
        "\n",
        "  for tweet in train_data:\n",
        "    train_tweets.append(Tweet(tweet[1],tweet[2],tweet[4]))\n",
        "\n",
        "  for tweet in test_data:\n",
        "    test_tweets.append(Tweet(tweet[1],tweet[2],tweet[4]))\n",
        "\n",
        "  return build_tree_id3(train_tweets,['keyword','location'],'target')"
      ],
      "metadata": {
        "id": "NRkVcDFtEFYW"
      },
      "execution_count": 501,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tree = tree(train_data,test_data)\n",
        "\n",
        "predictedOutput(tree,tweets[100])\n",
        "print(\"Predicted Value: \",value)\n",
        "print(\"Actual Value: \", tweets[100].target)\n",
        "\n",
        "predictedOutput(tree,tweets[40])\n",
        "print(\"Predicted Value: \",value)\n",
        "print(\"Actual Value: \", tweets[40].target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pn-duNfhCFwM",
        "outputId": "d091c507-d9f6-48c6-c77e-fe85f3010a06"
      },
      "execution_count": 502,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Value:  0\n",
            "Actual Value:  0\n",
            "Predicted Value:  1\n",
            "Actual Value:  1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def decision_tree_accuracies(tree,train_tweets,test_tweets):\n",
        "\n",
        "\n",
        "  one = accuracy(tree,train_tweets)\n",
        "  two = accuracy(decision_tree,test_tweets)\n",
        "\n",
        "  return(one,two)\n"
      ],
      "metadata": {
        "id": "9Ikri9xF8yUb"
      },
      "execution_count": 498,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_acc,test_acc = (decision_tree_accuracies(tree,train_tweets,test_tweets))\n",
        "print(\"train acc:\",train_acc)\n",
        "print(\"test acc:\",test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9td04GmG9vS",
        "outputId": "93421b6d-6d95-488d-b12a-eec173933a39"
      },
      "execution_count": 499,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train acc: 0.918\n",
            "test acc: 0.8396479142747799\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Forests"
      ],
      "metadata": {
        "id": "-Ph21W8syGLn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Shuffle and Split Data"
      ],
      "metadata": {
        "id": "9ugk1vcA674h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://jax.readthedocs.io/en/latest/_autosummary/jax.random.shuffle.html"
      ],
      "metadata": {
        "id": "kFbHn-C523ii"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def shuffle(array):\n",
        "  array = np.random.permutation(array)\n",
        "  return array\n",
        "\n",
        "def split_data(data):\n",
        "  global test_data, train_data\n",
        "  test_data = data[5000:]\n",
        "  train_data = data[:5000]\n",
        "\n",
        "def shuffle_and_split(data):\n",
        "  data = shuffle(data)\n",
        "  split_data(data)\n",
        "  \n",
        "shuffle_and_split(data)"
      ],
      "metadata": {
        "id": "I7MMoM-XlkZq"
      },
      "execution_count": 494,
      "outputs": []
    }
  ]
}