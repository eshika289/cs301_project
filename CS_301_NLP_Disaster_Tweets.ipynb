{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS 301 - NLP - Disaster Tweets",
      "provenance": [],
      "collapsed_sections": [
        "5OEH3tR8n8XD",
        "Ws-LiX-KoDGj",
        "_589ROqa4BQc",
        "QAcN9HpkxYh_",
        "41dT_a1xvdva",
        "E6iYA5pgwEd5",
        "smAoYiMPyB-N"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPxOt2DzTZ1SjcBhg2GbPt3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eshika289/cs301_project/blob/main/CS_301_NLP_Disaster_Tweets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.numpy as jnp\n",
        "from jax import grad, jit, vmap\n",
        "from jax import random"
      ],
      "metadata": {
        "id": "APL0P06erOf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://jax.readthedocs.io/en/latest/notebooks/quickstart.html"
      ],
      "metadata": {
        "id": "aHrLY07ZwuRq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, shutil\n",
        "import csv\n",
        "import re\n",
        "import math\n",
        "from collections import Counter\n",
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "XeGKCQKjxAD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from typing import Any\n",
        "from typing import Dict\n",
        "from typing import TypeVar"
      ],
      "metadata": {
        "id": "U4NCEisb0m2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet as wn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42_ZfksrqZ9F",
        "outputId": "990dbd14-121c-4271-b4c7-5ad1d955eef5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get Data From Kaggle"
      ],
      "metadata": {
        "id": "5OEH3tR8n8XD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get kaggle.json file from kaggle and upload it to the content folder"
      ],
      "metadata": {
        "id": "ptMx-wYPrgqr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd # /content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ak2cGh9prm06",
        "outputId": "09b5b022-113f-40c3-8b55-4148b608e11b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /root/.kaggle\n",
        "!mkdir kaggle"
      ],
      "metadata": {
        "id": "03d8Hklgrpxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mv ./kaggle.json /root/.kaggle/kaggle.json\n",
        "!cat /root/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "z51VyhCursMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install kaggle\n",
        "!kaggle -v"
      ],
      "metadata": {
        "id": "y1g6tHWXslah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod 600 /root/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "KuQSB3IGsr7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions files -c nlp-getting-started"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCqsnXy_tE3M",
        "outputId": "75b4ae5a-00cd-4abc-92fc-1a94d4524358"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name                    size  creationDate         \n",
            "---------------------  -----  -------------------  \n",
            "sample_submission.csv   22KB  2019-12-16 20:36:20  \n",
            "test.csv               411KB  2019-12-16 20:36:20  \n",
            "train.csv              965KB  2019-12-16 20:36:20  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions download -c nlp-getting-started -p /content/kaggle/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f43IO7CTsxjK",
        "outputId": "214ced55-b172-4aa0-be72-f89068546c7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading nlp-getting-started.zip to /content/kaggle\n",
            "\r  0% 0.00/593k [00:00<?, ?B/s]\n",
            "\r100% 593k/593k [00:00<00:00, 90.1MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q /content/kaggle/nlp-getting-started.zip -d /content/kaggle/data"
      ],
      "metadata": {
        "id": "V3ET8MSetuk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = \"kaggle/data/train.csv\"\n",
        "data_file = open(data_path)\n",
        "read_file = csv.reader(data_file)\n",
        "headers = []\n",
        "headers = next(read_file)\n",
        "data = []\n",
        "for row in read_file:\n",
        "  data.append(row)"
      ],
      "metadata": {
        "id": "3FBaTgM2mrRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(headers) # ['id', 'keyword', 'location', 'text', 'target']\n",
        "print(data[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3IEM4-mmxxMX",
        "outputId": "c9d70b2d-ddb1-4ff2-a7ea-61c060a94eb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['id', 'keyword', 'location', 'text', 'target']\n",
            "['1', '', '', 'Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all', '1']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Most Common Words as Features"
      ],
      "metadata": {
        "id": "Ws-LiX-KoDGj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def most_common_words(subset):\n",
        "  all_text = \"\"\n",
        "  for i in range(len(subset)):\n",
        "    all_text+=subset[i][3]+\" \"\n",
        "\n",
        "  stop_words = stopwords.words('english')\n",
        "  all_text.lower()\n",
        "  words_list = all_text.split()\n",
        "  word_list = []\n",
        "  part_of_speech = {}\n",
        "\n",
        "  for word in words_list:\n",
        "    if word not in stop_words and re.search('[a-z]', word) is not None and word!=\"&amp;\" and word[:4]!=\"http\":\n",
        "      #tmp = wn.synsets(word)[0].pos()\n",
        "      word_list.append(word.lower())\n",
        "      #part_of_speech[word.lower]= tmp\n",
        "\n",
        "  #words_dict = dict(sorted(words_dict.items(), key=lambda item: -item[1]))\n",
        "  words_dict = Counter(word_list)\n",
        "  for item in words_dict.most_common(50):\n",
        "    print(item)\n",
        "    #print(part_of_speech[item])\n",
        "\n",
        "most_common_words(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6YVXP_VfrV2b",
        "outputId": "a3d5d82e-7982-4e15-fc3b-4635e9566923"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('the', 552)\n",
            "('like', 335)\n",
            "(\"i'm\", 229)\n",
            "('get', 224)\n",
            "('via', 213)\n",
            "('fire', 203)\n",
            "('new', 203)\n",
            "('one', 178)\n",
            "('people', 168)\n",
            "('in', 155)\n",
            "('emergency', 135)\n",
            "('police', 125)\n",
            "('still', 125)\n",
            "('would', 124)\n",
            "('disaster', 118)\n",
            "('body', 116)\n",
            "('this', 110)\n",
            "('got', 108)\n",
            "('to', 108)\n",
            "('burning', 108)\n",
            "('video', 107)\n",
            "('suicide', 107)\n",
            "('see', 102)\n",
            "('first', 100)\n",
            "('back', 100)\n",
            "('know', 99)\n",
            "('going', 99)\n",
            "('man', 96)\n",
            "('you', 96)\n",
            "('storm', 96)\n",
            "('my', 95)\n",
            "('two', 95)\n",
            "('killed', 95)\n",
            "('love', 91)\n",
            "('time', 91)\n",
            "('california', 90)\n",
            "('bomb', 90)\n",
            "('if', 89)\n",
            "('nuclear', 89)\n",
            "('full', 88)\n",
            "('may', 87)\n",
            "('buildings', 87)\n",
            "('go', 86)\n",
            "('families', 86)\n",
            "(\"can't\", 85)\n",
            "('crash', 84)\n",
            "('train', 84)\n",
            "('car', 83)\n",
            "('@youtube', 83)\n",
            "('day', 83)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Keywords as Features"
      ],
      "metadata": {
        "id": "_589ROqa4BQc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "keywords = defaultdict(int)\n",
        "for keyword in [data[x][1] for x in range(len(data))]:\n",
        "  keywords[str(keyword)]+=1\n",
        "\n",
        "keywords = {k: v for k, v in sorted(keywords.items(), key=lambda item: -item[1])}\n",
        "\n",
        "print(keywords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0XHElRpw4Fej",
        "outputId": "92b41b62-a793-4656-9be0-3a5cf1e7f7ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'': 61, 'fatalities': 45, 'armageddon': 42, 'deluge': 42, 'body%20bags': 41, 'damage': 41, 'harm': 41, 'sinking': 41, 'collided': 40, 'evacuate': 40, 'fear': 40, 'outbreak': 40, 'siren': 40, 'twister': 40, 'windstorm': 40, 'collision': 39, 'derailment': 39, 'earthquake': 39, 'explosion': 39, 'famine': 39, 'flames': 39, 'hellfire': 39, 'sinkhole': 39, 'sunk': 39, 'weapon': 39, 'weapons': 39, 'whirlwind': 39, 'wreckage': 39, 'wrecked': 39, 'ambulance': 38, 'blaze': 38, 'bombed': 38, 'deaths': 38, 'derailed': 38, 'drowned': 38, 'explode': 38, 'fatal': 38, 'fire': 38, 'flooding': 38, 'hurricane': 38, 'injury': 38, 'oil%20spill': 38, 'thunder': 38, 'typhoon': 38, 'upheaval': 38, 'bioterror': 37, 'blizzard': 37, 'burning%20buildings': 37, 'crush': 37, 'curfew': 37, 'debris': 37, 'destroy': 37, 'emergency': 37, 'fatality': 37, 'hostages': 37, 'mudslide': 37, 'pandemonium': 37, 'panic': 37, 'police': 37, 'quarantined': 37, 'ruin': 37, 'sandstorm': 37, 'wounded': 37, 'wreck': 37, 'ablaze': 36, 'attack': 36, 'catastrophe': 36, 'cliff%20fall': 36, 'danger': 36, 'death': 36, 'desolation': 36, 'detonate': 36, 'devastation': 36, 'displaced': 36, 'dust%20storm': 36, 'engulfed': 36, 'evacuated': 36, 'evacuation': 36, 'floods': 36, 'massacre': 36, 'nuclear%20reactor': 36, 'refugees': 36, 'screaming': 36, 'tragedy': 36, 'accident': 35, 'airplane%20accident': 35, 'attacked': 35, 'bleeding': 35, 'blood': 35, 'bloody': 35, 'bridge%20collapse': 35, 'buildings%20burning': 35, 'casualties': 35, 'collapsed': 35, 'demolition': 35, 'derail': 35, 'disaster': 35, 'drought': 35, 'emergency%20plan': 35, 'flood': 35, 'hail': 35, 'hazardous': 35, 'hijacker': 35, 'injured': 35, 'inundated': 35, 'razed': 35, 'rescued': 35, 'rescuers': 35, 'rioting': 35, 'screams': 35, 'storm': 35, 'structural%20failure': 35, 'suicide%20bomb': 35, 'tornado': 35, 'traumatised': 35, 'aftershock': 34, 'annihilated': 34, 'army': 34, 'arsonist': 34, 'blazing': 34, 'bomb': 34, 'burning': 34, 'casualty': 34, 'collapse': 34, 'collide': 34, 'crashed': 34, 'demolish': 34, 'destruction': 34, 'drowning': 34, 'electrocuted': 34, 'flattened': 34, 'hazard': 34, 'heat%20wave': 34, 'lava': 34, 'loud%20bang': 34, 'military': 34, 'natural%20disaster': 34, 'nuclear%20disaster': 34, 'quarantine': 34, 'rainstorm': 34, 'riot': 34, 'screamed': 34, 'smoke': 34, 'terrorism': 34, 'tsunami': 34, 'blew%20up': 33, 'blown%20up': 33, 'body%20bag': 33, 'body%20bagging': 33, 'buildings%20on%20fire': 33, 'burned': 33, 'chemical%20emergency': 33, 'crash': 33, 'emergency%20services': 33, 'exploded': 33, 'fire%20truck': 33, 'hijack': 33, 'injuries': 33, 'landslide': 33, 'lightning': 33, 'mass%20murder': 33, 'meltdown': 33, 'panicking': 33, 'stretcher': 33, 'suicide%20bombing': 33, 'survived': 33, 'thunderstorm': 33, 'trouble': 33, 'violent%20storm': 33, 'wildfire': 33, 'wounds': 33, 'apocalypse': 32, 'arson': 32, 'blight': 32, 'cyclone': 32, 'destroyed': 32, 'detonation': 32, 'drown': 32, 'electrocute': 32, 'eyewitness': 32, 'forest%20fires': 32, 'hailstorm': 32, 'hijacking': 32, 'mass%20murderer': 32, 'survive': 32, 'trapped': 32, 'crushed': 31, 'devastated': 31, 'hostage': 31, 'obliterate': 31, 'obliterated': 31, 'suicide%20bomber': 31, 'terrorist': 31, 'trauma': 31, 'wild%20fires': 31, 'avalanche': 30, 'bioterrorism': 30, 'catastrophic': 30, 'dead': 30, 'mayhem': 30, 'survivors': 30, 'annihilation': 29, 'bombing': 29, 'desolate': 29, 'first%20responders': 29, 'obliteration': 29, 'seismic': 29, 'sirens': 29, 'snowstorm': 29, 'demolished': 28, 'rubble': 28, 'deluged': 27, 'volcano': 27, 'battle': 26, 'bush%20fires': 25, 'war%20zone': 24, 'rescue': 22, 'forest%20fire': 19, 'epicentre': 12, 'threat': 11, 'inundation': 10, 'radiation%20emergency': 9}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def by_keyword(word):\n",
        "  valid = 0\n",
        "  falseAlarm = 0\n",
        "  for item in data:\n",
        "    if item[1]==word:\n",
        "      if item[4]=='1':\n",
        "        valid+=1\n",
        "      else:\n",
        "        falseAlarm+=1\n",
        "  print(\"Valid : \" + str(valid))\n",
        "  print(\"False : \" + str(falseAlarm))\n",
        "\n",
        "def keyword_subset(word):\n",
        "  subset = []\n",
        "  for item in data:\n",
        "    if item[1]==word:\n",
        "      subset.append(item)\n",
        "  return subset\n",
        "\n",
        "by_keyword(\"destroy\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KN8-1Vl62xW",
        "outputId": "131e7f34-06fe-469b-aa57-36668a97f4cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Valid : 9\n",
            "False : 28\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Words that Come Before/After Keywords"
      ],
      "metadata": {
        "id": "QAcN9HpkxYh_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ZCNBYlCbxgjX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Locations As Features"
      ],
      "metadata": {
        "id": "41dT_a1xvdva"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def by_location(location): \n",
        "  locations_v = []\n",
        "  locations_f = []\n",
        "  for item in data:\n",
        "    if item[4]==\"1\":\n",
        "      locations_v.append(item[2])\n",
        "    else:\n",
        "      locations_f.append(item[2])\n",
        "  locations_v = Counter(locations_v)\n",
        "  locations_f = Counter(locations_f)\n",
        "  print(str(location) + \": Valid : \" + str(locations_v[location]))\n",
        "  print(str(location) + \": False : \" + str(locations_f[location]))"
      ],
      "metadata": {
        "id": "OLP7Bco8vguD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bigrams / Trigrams As Features"
      ],
      "metadata": {
        "id": "E6iYA5pgwEd5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "O87hFa0FwDXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Grammars (nouns,verbs) As Features"
      ],
      "metadata": {
        "id": "BcEqjwo9xt4p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "GHEcwKh6yAes"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Tree"
      ],
      "metadata": {
        "id": "smAoYiMPyB-N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entropy"
      ],
      "metadata": {
        "id": "Lf8CA4ZozCcF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def entropy(class_probabilities: List[float]) -> float:\n",
        "  '''Given a list of class probabilities, compute the entropy'''\n",
        "  return sum(-p*math.log(p,2) for p in class_probabilities if p > 0)\n",
        "\n",
        "def class_probabilities(labels: List[Any]) -> float:\n",
        "  total_count = len(labels)\n",
        "  return [count/total_count for count in Counter(labels).values()]\n",
        "\n",
        "def data_entropy(labels: List[Any]) -> float:\n",
        "  return entropy(class_probabilities(labels))\n",
        "\n",
        "def partition_entropy(subsets: List[List[Any]]) -> float:\n",
        "  '''Returns the entropy from this partition of data into subsets'''\n",
        "  total_count = sum(len(subset) for subset in subsets)\n",
        "  return sum(data_entropy(subset)*len(subset)/total_count for subset in subsets)"
      ],
      "metadata": {
        "id": "g3jcrUV4yxrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parition"
      ],
      "metadata": {
        "id": "-rArfTFo2I7Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "T = TypeVar('T')\n",
        "\n",
        "def partition_by(inputs: List[T], attribute:str) -> Dict[Any, List[T]]:\n",
        "  '''Partitions the inputs into lists based on the specified attribute'''\n",
        "  partitions: Dict[Any,List[T]] = defaultdict(list)\n",
        "  for input in inputs:\n",
        "    key = getattr(input, attribute)\n",
        "    partitions[key].append(input)\n",
        "  return partitions\n",
        "\n",
        "def partition_entropy_by(inputs: List[Any], attribute:str,label_attribute:str) ->float:\n",
        "  '''Computes the entropy corresponding to the given partition'''\n",
        "  partitions = partition_by(inputs,attribute)\n",
        "  labels = [[getattr(input, label_attribute) for input in partition] for partition in partitions.values()]\n",
        "  return partition_entropy(labels)"
      ],
      "metadata": {
        "id": "TUFMpSL72HAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build Tree"
      ],
      "metadata": {
        "id": "-na0Idv44SQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "4jUBKAeD4R41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Forests"
      ],
      "metadata": {
        "id": "-Ph21W8syGLn"
      }
    }
  ]
}