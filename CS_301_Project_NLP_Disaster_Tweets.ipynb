{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS 301 Project - NLP - Disaster Tweets",
      "provenance": [],
      "collapsed_sections": [
        "3MZjlumnpEKm",
        "5OEH3tR8n8XD",
        "1HvJN22spr7-",
        "Ws-LiX-KoDGj",
        "E6iYA5pgwEd5",
        "Lf8CA4ZozCcF",
        "-rArfTFo2I7Y",
        "vgNrFYDM1vVr",
        "-na0Idv44SQO",
        "-sWStJ_P9_lf"
      ],
      "authorship_tag": "ABX9TyNHGIswWbu+5LKPcDLDyAv4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eshika289/cs301_project/blob/main/CS_301_Project_NLP_Disaster_Tweets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Statements"
      ],
      "metadata": {
        "id": "3MZjlumnpEKm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.numpy as jnp\n",
        "from jax import grad, jit, vmap\n",
        "from jax import random\n",
        "import jax"
      ],
      "metadata": {
        "id": "APL0P06erOf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://jax.readthedocs.io/en/latest/notebooks/quickstart.html"
      ],
      "metadata": {
        "id": "aHrLY07ZwuRq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, shutil\n",
        "import csv\n",
        "import re\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "XeGKCQKjxAD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Union\n",
        "from typing import Any\n",
        "from typing import Dict\n",
        "from typing import TypeVar\n",
        "from typing import NamedTuple, Optional"
      ],
      "metadata": {
        "id": "U4NCEisb0m2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import wordnet as wn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42_ZfksrqZ9F",
        "outputId": "aa50bf93-3b1f-42e4-e341-d05bb22389e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get Data From Kaggle"
      ],
      "metadata": {
        "id": "5OEH3tR8n8XD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get kaggle.json file from kaggle and upload it to the content folder"
      ],
      "metadata": {
        "id": "ptMx-wYPrgqr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd # /content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ak2cGh9prm06",
        "outputId": "9cd09e95-a606-4485-bc58-3e495d02de5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /root/.kaggle\n",
        "!mkdir kaggle"
      ],
      "metadata": {
        "id": "03d8Hklgrpxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv ./kaggle.json /root/.kaggle/kaggle.json\n",
        "!cat /root/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "z51VyhCursMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install kaggle\n",
        "!kaggle -v"
      ],
      "metadata": {
        "id": "y1g6tHWXslah",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b775d94d-a60e-4ba3-edf0-0a5d92c7bfd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "Kaggle API 1.5.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod 600 /root/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "KuQSB3IGsr7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions files -c nlp-getting-started"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCqsnXy_tE3M",
        "outputId": "f3787ff9-f2b5-46a4-9db0-9f039bf8edb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name                    size  creationDate         \n",
            "---------------------  -----  -------------------  \n",
            "sample_submission.csv   22KB  2019-12-16 20:36:20  \n",
            "test.csv               411KB  2019-12-16 20:36:20  \n",
            "train.csv              965KB  2019-12-16 20:36:20  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions download -c nlp-getting-started -p /content/kaggle/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f43IO7CTsxjK",
        "outputId": "f00cb50b-ed3b-4e04-a753-cfcda31c3e48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading nlp-getting-started.zip to /content/kaggle\n",
            "\r  0% 0.00/593k [00:00<?, ?B/s]\n",
            "\r100% 593k/593k [00:00<00:00, 67.3MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q /content/kaggle/nlp-getting-started.zip -d /content/kaggle/data"
      ],
      "metadata": {
        "id": "V3ET8MSetuk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = \"kaggle/data/train.csv\"\n",
        "data_file = open(data_path)\n",
        "read_file = csv.reader(data_file)\n",
        "headers = []\n",
        "headers = next(read_file)\n",
        "data = []\n",
        "for row in read_file:\n",
        "  data.append(row)"
      ],
      "metadata": {
        "id": "3FBaTgM2mrRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(headers) # ['id', 'keyword', 'location', 'text', 'target']\n",
        "print(data[0])\n",
        "print(len(data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3IEM4-mmxxMX",
        "outputId": "7686eed6-1ee5-4247-c570-53c69e827549"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['id', 'keyword', 'location', 'text', 'target']\n",
            "['1', '', '', 'Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all', '1']\n",
            "7613\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def shuffle(array):\n",
        "  array = np.random.permutation(array)\n",
        "  return array\n",
        "\n",
        "def split_data(data):\n",
        "  global test_data, train_data\n",
        "  test_data = data[5000:]\n",
        "  train_data = data[:5000]\n",
        "\n",
        "def shuffle_and_split(data):\n",
        "  data = shuffle(data)\n",
        "  split_data(data)\n",
        "  \n",
        "shuffle_and_split(data)"
      ],
      "metadata": {
        "id": "lAls_o5m6WEO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Features"
      ],
      "metadata": {
        "id": "1HvJN22spr7-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Most Common Words as Features"
      ],
      "metadata": {
        "id": "Ws-LiX-KoDGj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def most_common_words(subset):\n",
        "  all_text = \"\"\n",
        "  for i in range(len(subset)):\n",
        "    all_text+=subset[i][3]+\" \"\n",
        "\n",
        "  stop_words = stopwords.words('english')\n",
        "  all_text.lower()\n",
        "  words_list = all_text.split()\n",
        "  word_list = []\n",
        "  part_of_speech = {}\n",
        "\n",
        "  for word in words_list:\n",
        "    if word not in stop_words and re.search('[a-z]', word) is not None and word!=\"&amp;\" and word[:4]!=\"http\":\n",
        "      #tmp = wn.synsets(word)[0].pos()\n",
        "      word_list.append(word.lower())\n",
        "      #part_of_speech[word.lower]= tmp\n",
        "\n",
        "  #words_dict = dict(sorted(words_dict.items(), key=lambda item: -item[1]))\n",
        "  words_dict = Counter(word_list)\n",
        "  for item in words_dict.most_common(50):\n",
        "    print(item)\n",
        "    #print(part_of_speech[item])\n",
        "\n",
        "most_common_words(data)"
      ],
      "metadata": {
        "id": "6YVXP_VfrV2b",
        "outputId": "d692803c-f690-4a6a-aa1d-e939eb1c3d44",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('the', 552)\n",
            "('like', 335)\n",
            "(\"i'm\", 229)\n",
            "('get', 224)\n",
            "('via', 213)\n",
            "('fire', 203)\n",
            "('new', 203)\n",
            "('one', 178)\n",
            "('people', 168)\n",
            "('in', 155)\n",
            "('emergency', 135)\n",
            "('police', 125)\n",
            "('still', 125)\n",
            "('would', 124)\n",
            "('disaster', 118)\n",
            "('body', 116)\n",
            "('this', 110)\n",
            "('got', 108)\n",
            "('to', 108)\n",
            "('burning', 108)\n",
            "('video', 107)\n",
            "('suicide', 107)\n",
            "('see', 102)\n",
            "('first', 100)\n",
            "('back', 100)\n",
            "('know', 99)\n",
            "('going', 99)\n",
            "('man', 96)\n",
            "('you', 96)\n",
            "('storm', 96)\n",
            "('my', 95)\n",
            "('two', 95)\n",
            "('killed', 95)\n",
            "('love', 91)\n",
            "('time', 91)\n",
            "('california', 90)\n",
            "('bomb', 90)\n",
            "('if', 89)\n",
            "('nuclear', 89)\n",
            "('full', 88)\n",
            "('may', 87)\n",
            "('buildings', 87)\n",
            "('go', 86)\n",
            "('families', 86)\n",
            "(\"can't\", 85)\n",
            "('crash', 84)\n",
            "('train', 84)\n",
            "('car', 83)\n",
            "('@youtube', 83)\n",
            "('day', 83)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bigrams / Trigrams As Features"
      ],
      "metadata": {
        "id": "E6iYA5pgwEd5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "O87hFa0FwDXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grammars (nouns,verbs) As Features"
      ],
      "metadata": {
        "id": "BcEqjwo9xt4p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "GHEcwKh6yAes"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Tree"
      ],
      "metadata": {
        "id": "smAoYiMPyB-N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entropy"
      ],
      "metadata": {
        "id": "Lf8CA4ZozCcF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def entropy(class_probabilities: List[float]) -> float:\n",
        "  '''Given a list of class probabilities, compute the entropy'''\n",
        "  return jnp.sum(jnp.asarray([-p*jnp.log2(p) for p in class_probabilities if p > 0]))\n",
        "\n",
        "def class_probabilities(labels: List[Any]) -> float:\n",
        "  total_count = jnp.size(labels)\n",
        "  return [count/total_count for count in Counter(labels).values()]\n",
        "\n",
        "def data_entropy(labels: List[Any]) -> float:\n",
        "  return entropy(class_probabilities(labels))\n",
        "\n",
        "def partition_entropy(subsets: List[List[Any]]) -> float:\n",
        "  '''Returns the entropy from this partition of data into subsets'''\n",
        "  total_count = jnp.sum(jnp.asarray([jnp.size(subset) for subset in subsets]))\n",
        "  return jnp.sum(jnp.asarray([data_entropy(subset)*jnp.size(subset)/total_count for subset in subsets]))"
      ],
      "metadata": {
        "id": "g3jcrUV4yxrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parition"
      ],
      "metadata": {
        "id": "-rArfTFo2I7Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "T = TypeVar('T')\n",
        "\n",
        "def partition_by(inputs: List[T], attribute:str) -> Dict[Any, List[T]]:\n",
        "  '''Partitions the inputs into lists based on the specified attribute'''\n",
        "  partitions: Dict[Any,List[T]] = defaultdict(list)\n",
        "  for input in inputs:\n",
        "    key = getattr(input, attribute)\n",
        "    partitions[key].append(input)\n",
        "  return partitions\n",
        "\n",
        "def partition_entropy_by(inputs: List[Any], attribute:str,label_attribute:str) ->float:\n",
        "  '''Computes the entropy corresponding to the given partition'''\n",
        "  partitions = partition_by(inputs,attribute)\n",
        "  labels = [[getattr(input, label_attribute) for input in partition] for partition in partitions.values()]\n",
        "  return partition_entropy(labels)"
      ],
      "metadata": {
        "id": "TUFMpSL72HAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Entropy and Partition"
      ],
      "metadata": {
        "id": "vgNrFYDM1vVr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(headers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9q262QITwIi",
        "outputId": "ed3f1fd8-38f9-4faf-b73f-078d3ac2f447"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['id', 'keyword', 'location', 'text', 'target']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweets = []\n",
        "\n",
        "class Tweet(NamedTuple):\n",
        "  keyword: str\n",
        "  location: str\n",
        "  target: int\n",
        "\n",
        "for tweet in train_data:\n",
        "  tweets.append(Tweet(tweet[1],tweet[2],tweet[4]))"
      ],
      "metadata": {
        "id": "zM_g0CyWTqQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(tweets))\n",
        "print(len(train_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DxHy7u2mVyXA",
        "outputId": "a3daca18-c742-4c71-fb4d-8cfc37bfecd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5000\n",
            "5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for key in ['keyword','location']:\n",
        "  print(key,partition_entropy_by(tweets,key,'target'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxuyEKSQWSYr",
        "outputId": "34db8700-0759-4438-98fc-fcfa97c0b9dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "keyword 0.71865314\n",
            "location 0.5042092\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build Tree"
      ],
      "metadata": {
        "id": "-na0Idv44SQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Leaf(NamedTuple):\n",
        "  value: Any\n",
        "\n",
        "class Split(NamedTuple):\n",
        "  attribute: str\n",
        "  subtrees: dict\n",
        "  default_value: Any = None\n",
        "\n",
        "DecisionTree = Union[Leaf,Split]"
      ],
      "metadata": {
        "id": "454J3DuR28o8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Tweet(NamedTuple):\n",
        "  keyword: str\n",
        "  location: str\n",
        "  target: int"
      ],
      "metadata": {
        "id": "uhZgO9ux9Svn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classify(tree:DecisionTree,input:Any) -> Any:\n",
        "  '''Classify the input using the given decision tree'''\n",
        "  \n",
        "  # If this is a leaf node, return its value\n",
        "  if isinstance(tree,Leaf):\n",
        "    return tree.value\n",
        "\n",
        "  subtree_key = getattr(input, tree.attribute)\n",
        "\n",
        "  if subtree_key not in tree.subtrees:\n",
        "    return tree.default_value\n",
        "  \n",
        "  subtree = tree.subtrees[subtree_key]\n",
        "  return classify(subtree,input)"
      ],
      "metadata": {
        "id": "nDiHJANM4BrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_tree_id3(inputs: List[Any],split_attributes:List[str],target_attribute:str)->DecisionTree:\n",
        "  # Count target labels\n",
        "  label_counts = Counter(getattr(input,target_attribute) for input in inputs)\n",
        "  most_common_label = label_counts.most_common(1)[0][0]\n",
        "\n",
        "  # If there's a unique label, predict it\n",
        "  if len(label_counts) == 1:\n",
        "    return Leaf(most_common_label)\n",
        "  \n",
        "  # If no split attributes left, return the majority label\n",
        "  if not split_attributes:\n",
        "    return Leaf(most_common_label)\n",
        "\n",
        "  # Otherwise split by the best attribute\n",
        "  def split_entropy(attribute:str) -> float:\n",
        "    return partition_entropy_by(inputs,attribute,target_attribute)\n",
        "  \n",
        "  best_attribute = min(split_attributes, key=split_entropy)\n",
        "\n",
        "  partitions = partition_by(inputs, best_attribute)\n",
        "  new_attributes = [a for a in split_attributes if a !=best_attribute]\n",
        "\n",
        "  # Recursively build subtrees\n",
        "  subtrees = {attribute_value: build_tree_id3(subset,new_attributes, target_attribute) for attribute_value,subset in partitions.items()}\n",
        "\n",
        "  return Split(best_attribute, subtrees, default_value=most_common_label)"
      ],
      "metadata": {
        "id": "QcWju7rK5sEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Build Tree"
      ],
      "metadata": {
        "id": "-sWStJ_P9_lf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decision_tree = build_tree_id3(tweets,['keyword','location'],'target')\n",
        "#print(decision_tree)"
      ],
      "metadata": {
        "id": "zFJILcu2WlTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Accuracy of Tree"
      ],
      "metadata": {
        "id": "KYOXMOUHkgQe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(decision_tree,tweets):\n",
        "  correct = 0\n",
        "  incorrect = 0\n",
        "  for tweet in tweets:\n",
        "    predictedOutput(decision_tree,tweet)\n",
        "    if value== tweet.target:\n",
        "      correct+=1\n",
        "    else:\n",
        "      incorrect+=1\n",
        "  #print(\"Correct: \", correct)\n",
        "  #print(\"Incorrect: \", incorrect)\n",
        "\n",
        "  accuracy = correct/(correct+incorrect)\n",
        "  #print(\"Accuracy: \", accuracy)\n",
        "  return accuracy\n",
        "\n",
        "\n",
        "def predictedOutput(decision_tree,tweet):\n",
        "  ''' will use the decision_tree to predict if tweet is valid or not\n",
        "      changes the global variable value to 1 for valid and 0 for invalid\n",
        "  '''\n",
        "  global value\n",
        "  tree= decision_tree\n",
        "  attribute = tree[0]\n",
        "  tweet_val = None\n",
        "  if attribute == 'location':\n",
        "    tweet_val = tweet.location\n",
        "  if attribute == 'keyword':\n",
        "    tweet_val = tweet.keyword\n",
        "  try:\n",
        "    tree = tree[1][tweet_val]\n",
        "    if isinstance(tree,Leaf):\n",
        "        value = tree.value\n",
        "    else:\n",
        "      predictedOutput(tree,tweet)\n",
        "  except:\n",
        "    value = tree[2]"
      ],
      "metadata": {
        "id": "8cmwiNChd09V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy(decision_tree,tweets)\n",
        "#predictedOutput(decision_tree,tweets[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQ81UTqSpb_I",
        "outputId": "a38baac0-04df-4336-e8fb-f36231447d4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9246"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Dataset Accuracy"
      ],
      "metadata": {
        "id": "wKVM3KTDkpyT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_tweets = []\n",
        "for tweet in test_data:\n",
        "  test_tweets.append(Tweet(tweet[1],tweet[2],tweet[4]))\n",
        "\n",
        "print(len(test_data))\n",
        "print(len(test_tweets))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vD4_exIWm7dp",
        "outputId": "9568c1aa-5648-4336-91bc-f592d23754eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2613\n",
            "2613\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy(decision_tree,test_tweets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-E2kh6tlm1gi",
        "outputId": "4b4199da-ef8c-4e2e-ac93-874daaca9225"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6398775353999234"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions to Use to Build Trees "
      ],
      "metadata": {
        "id": "kVVNPZ-sD7n3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   tree(train_data,test_data) -> returns a decision tree\n",
        "*   predictedOutput(tree,tweet) -> changes the global variable value to 1 if valid and 0 if not valid\n",
        "*   decision_tree_accuracies(tree,train_tweets,test_tweets) -> returns the train and test accuracies as a tuple (train_accuracy,test_accuracy)\n"
      ],
      "metadata": {
        "id": "pvPt757vFRyk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tree(train_data,test_data):\n",
        "  global train_tweets\n",
        "  global test_tweets\n",
        "  train_tweets = []\n",
        "  test_tweets = []\n",
        "\n",
        "  for tweet in train_data:\n",
        "    train_tweets.append(Tweet(tweet[1],tweet[2],tweet[4]))\n",
        "\n",
        "  for tweet in test_data:\n",
        "    test_tweets.append(Tweet(tweet[1],tweet[2],tweet[4]))\n",
        "\n",
        "  return build_tree_id3(train_tweets,['keyword','location'],'target')"
      ],
      "metadata": {
        "id": "NRkVcDFtEFYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tree = tree(train_data,test_data)\n",
        "\n",
        "predictedOutput(tree,tweets[100])\n",
        "print(\"Predicted Value: \",value)\n",
        "print(\"Actual Value: \", tweets[100].target)\n",
        "\n",
        "predictedOutput(tree,tweets[40])\n",
        "print(\"Predicted Value: \",value)\n",
        "print(\"Actual Value: \", tweets[40].target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pn-duNfhCFwM",
        "outputId": "df2236c4-f7a7-444d-ce0e-ac7d4045447d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Value:  1\n",
            "Actual Value:  1\n",
            "Predicted Value:  1\n",
            "Actual Value:  1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def decision_tree_accuracies(tree,train_tweets,test_tweets):\n",
        "\n",
        "\n",
        "  one = accuracy(tree,train_tweets)\n",
        "  two = accuracy(decision_tree,test_tweets)\n",
        "\n",
        "  return(one,two)\n"
      ],
      "metadata": {
        "id": "9Ikri9xF8yUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_acc,test_acc = (decision_tree_accuracies(tree,train_tweets,test_tweets))\n",
        "print(\"train acc:\",train_acc)\n",
        "print(\"test acc:\",test_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9td04GmG9vS",
        "outputId": "b8f86449-5340-40b7-9d17-e178274a3393"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train acc: 0.9246\n",
            "test acc: 0.6398775353999234\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Forests"
      ],
      "metadata": {
        "id": "-Ph21W8syGLn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Shuffle and Split Data"
      ],
      "metadata": {
        "id": "9ugk1vcA674h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://jax.readthedocs.io/en/latest/_autosummary/jax.random.shuffle.html"
      ],
      "metadata": {
        "id": "kFbHn-C523ii"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "shuffle_and_split(data)"
      ],
      "metadata": {
        "id": "I7MMoM-XlkZq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decision_tree = build_tree_id3(tweets,['keyword','location'],'target')"
      ],
      "metadata": {
        "id": "wdWpolNE6uGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy(decision_tree,tweets)\n",
        "#predictedOutput(decision_tree,tweets[0])"
      ],
      "metadata": {
        "id": "Ai3F53W065Xs",
        "outputId": "f17871f2-807a-49be-bc64-694a09716e7f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9246"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_tweets = []\n",
        "for tweet in test_data:\n",
        "  test_tweets.append(Tweet(tweet[1],tweet[2],tweet[4]))\n",
        "accuracy(decision_tree,test_tweets)"
      ],
      "metadata": {
        "id": "ZX3SSRbV7Cvu",
        "outputId": "678273f9-3e30-4567-ae25-dd70115c64fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.831611174894757"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    }
  ]
}